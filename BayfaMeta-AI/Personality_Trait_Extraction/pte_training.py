# -*- coding: utf-8 -*-
"""PTE-Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Apno4WzoMzGc-yt9eriWMR7A2UqMVgKV
"""

from google.colab import drive

drive.mount("/content/drive")

from keras.callbacks import EarlyStopping
from keras.applications import vgg16
from keras.layers import Conv2D, MaxPooling2D, TimeDistributed, BatchNormalization, Dropout, Dense, Flatten, Input, LSTM, Concatenate
from keras.models import Model, model_from_json
from keras.regularizers import l1
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import os
import os.path as osp
import matplotlib.pyplot as plt

def plot_history(history):
    """
    Plot the history of the model.
    :param history: The history of the model.
    """

    plt.figure(figsize=(9, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['mae'])
    plt.plot(history.history['val_mae'])
    plt.title('Model MAE')
    plt.ylabel('MAE')
    plt.xlabel('Epoch')
    plt.legend(['Training MAE', 'Validation MAE'], loc='upper right')


def plot_1_mae(one_minus_mae_train, one_minus_mae_val):
    """
    Plot the 1-MAE of the model.
    :param one_minus_mae_train: The 1-MAE of the training set.
    :param one_minus_mae_val: The 1-MAE of the validation set.
    """

    plt.figure(figsize=(9, 4))
    plt.subplot(1, 2, 1)
    plt.plot(one_minus_mae_train)
    plt.plot(one_minus_mae_val)
    plt.title('Model 1-MAE')
    plt.ylabel('1-MAE')
    plt.xlabel('Epoch')
    plt.legend(['Train 1-MAE(Accuracy)', 'Validation 1-MAE(Accuracy)'], loc='lower right')


def evaluate(model):
    """
    Evaluate the model on the training set, validation set, and test set.
    :param model: The model to be evaluated.
    """

    train_loss, train_mae, train_mse = model.evaluate(
        [train_X0, train_X1], train_y, verbose=0)
    print("-" * 30)
    print('Train loss: {}'.format(train_loss))
    print('Train MAE: {}'.format(train_mae))
    print('Train MSE: {}'.format(train_mse))

    val_loss, val_mae, val_mse = model.evaluate(
        [val_X0, val_X1], val_y, verbose=0)

    print('\nValidation loss: {}'.format(val_loss))
    print('Validation MAE: {}'.format(val_mae))
    print('Validation MSE: {}'.format(val_mse))

    one_minus_mae_train = [1 - mae for mae in history.history['mae']]
    one_minus_mae_val = [1 - mae for mae in history.history['val_mae']]

    plot_1_mae(one_minus_mae_train, one_minus_mae_val)

    test_loss, test_mae, test_mse = model.evaluate(
        [test_X0, test_X1], test_y, verbose=0)

    print('\nTest loss: {}'.format(test_loss))
    print('Test MAE: {}'.format(test_mae))
    print('Test MSE: {}'.format(test_mse))
    print("-" * 30)

def load_dataset(data_path):
    """
    Load the datasets from the given npz file.

    Parameters:
    data_path (str): The path to the npz file containing the datasets.

    Returns:
    tuple: A tuple containing the following arrays:
           - X0: The training data for feature set 0.
           - X1: The training data for feature set 1.
           - y: The training labels.
           - X0_val: The validation data for feature set 0.
           - X1_val: The validation data for feature set 1.
           - y_val: The validation labels.
           - X0_test: The test data for feature set 0.
           - X1_test: The test data for feature set 1.
           - y_test: The test labels.
    """
    loaded_data = np.load(data_path)

    X0 = loaded_data['X0_train']
    X1 = loaded_data['X1_train']
    y = loaded_data['y_train']
    X0_val = loaded_data['X0_val']
    X1_val = loaded_data['X1_val']
    y_val = loaded_data['y_val']
    X0_test = loaded_data['X0_test']
    X1_test = loaded_data['X1_test']
    y_test = loaded_data['y_test']

    return X0, X1, y, X0_val, X1_val, y_val, X0_test, X1_test, y_test

# If the model has been trained, load the model
model_folder_path = '/content/drive/MyDrive/Automatic Recruitment System/model'
json_path = osp.join(model_folder_path, 'model.json')
weights_path = osp.join(model_folder_path, 'audio_video_weights.h5')
model_path = osp.join(model_folder_path, 'audio_video_model.h5')

# Create the folder if it does not exist
if not osp.exists(model_folder_path):
    os.makedirs(model_folder_path)

if osp.exists(json_path) and osp.exists(weights_path):
    cnn = vgg16.VGG16(weights="imagenet", include_top=False, pooling='max')
    cnn.trainable = False

    json_file = open(json_path, 'r')
    loaded_model_json = json_file.read()
    json_file.close()

    combined_network = model_from_json(loaded_model_json, custom_objects={'cnn':cnn})
    combined_network.load_weights(weights_path)

else:
    # Load training and validation data
    data_path = '/content/drive/MyDrive/Automatic Recruitment System/all_data.npz'
    train_X0, train_X1, train_y, val_X0, val_X1, val_y, test_X0, test_X1, test_y = load_dataset(data_path)

    # Build the model 
    audio_input = Input(shape=(24, 1319, 1))
    audio_model = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(audio_input)
    audio_model = BatchNormalization()(audio_model)
    audio_model = MaxPooling2D(pool_size=(2, 2))(audio_model)
    audio_model = Dropout(0.25)(audio_model)
    audio_model = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(audio_model)
    audio_model = BatchNormalization()(audio_model)
    audio_model = MaxPooling2D(pool_size=(2, 2))(audio_model)
    audio_model = Dropout(0.25)(audio_model)
    audio_model = Flatten()(audio_model)
    audio_model = Dense(128, activation='relu', kernel_regularizer=l1(0.01))(audio_model)
    audio_subnetwork = Model(inputs=audio_input, outputs=audio_model)

    visual_model = Input(shape=(6, 128, 128, 3))

    cnn = vgg16.VGG16(weights="imagenet", include_top=False, pooling='max')  
    cnn.trainable = False

    encoded_frame = TimeDistributed(cnn)(visual_model)
    encoded_vid = LSTM(64)(encoded_frame)

    visual_subnetwork = Model(inputs=visual_model, outputs=encoded_vid)

    combined = Concatenate()([audio_subnetwork.output, visual_subnetwork.output])
    final1 = Dense(256, activation='relu', kernel_regularizer=l1(0.01))(combined)
    final1 = Dropout(0.5)(final1)
    final2 = Dense(5, activation='linear')(final1)

    combined_network = Model(inputs=[audio_input, visual_model], outputs=final2)

    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)
    combined_network.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])

    early_stopping = EarlyStopping(monitor='val_loss', 
                                patience=5, 
                                restore_best_weights=True,
                                mode='min')

    # Train the model
    history = combined_network.fit(
        [train_X0, train_X1],
        train_y,
        validation_data=([val_X0, val_X1], val_y),
        epochs=100,
        verbose=1,
        callbacks=[early_stopping, lr_scheduler]
    )

    # Evaluate the model on the training and validation sets
    train_loss, train_mae, train_mse = combined_network.evaluate(
        [train_X0, train_X1], train_y, verbose=0)
    val_loss, val_mae, val_mse = combined_network.evaluate(
        [val_X0, val_X1], val_y, verbose=0)

    print("-" * 30)
    print('Train Loss: {}'.format(train_loss))
    print('Validation Loss: {}'.format(val_loss))
    print('Train MAE: {}'.format(train_mae))
    print('Validation MAE: {}'.format(val_mae))
    print('Train MSE: {}'.format(train_mse))
    print('Validation MSE: {}'.format(val_mse))
    print("-" * 30)

    model_json = combined_network.to_json()
    with open(json_path, "w") as json_file:
        json_file.write(model_json)

    combined_network.save_weights(weights_path)
    combined_network.save(model_path)

    print(history.history.keys())
    print("-" * 30)
    plot_history(history)

    evaluate(combined_network)

def evaluate_metrics(y_true, y_pred):
    """
    Evaluate the model on the test set using accuracy and R2 score.
    :param y_true: The true labels.
    :param y_pred: The predicted labels.
    """
    traits = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']
    for i in range(5):
        print('Trait: {}'.format(traits[i]))
        print('Mean absolute error: {}'.format(mean_absolute_error(y_true[:, i], y_pred[:, i])))
        print('Mean squared error: {}'.format(mean_squared_error(y_true[:, i], y_pred[:, i])))
        print('R2 score: {}'.format(r2_score(y_true[:, i], y_pred[:, i])))
        print("-" * 30)

# Prediction results
y_pred = combined_network.predict([test_X0, test_X1])
evaluate_metrics(test_y, y_pred)

# Plot the prediction results on the test set using scatter plot
pred_shape = y_pred.shape
y_pred = y_pred.reshape(pred_shape[0], pred_shape[1], 1)
norm_y_pred = y_pred / np.max(y_pred)
plt.scatter(test_y, norm_y_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.axis('equal')
# -*- coding: utf-8 -*-
"""PTE-Data Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQKsCiuJj-6HywYvfOr569L6xr6AhLDN
"""

from google.colab import drive

drive.mount("/content/drive")

!pip install ffmpeg-python
!pip install librosa

import librosa
import librosa.display
import numpy as np
import pandas as pd
import ffmpeg as ff
import os
import os.path as osp
from typing import List, Tuple
import matplotlib.pyplot as plt
import random
import cv2
import pickle
import datetime

def unzip_files():
  """
  Unzip the files if they are not unzipped yet.
  """
  if not(osp.exists('/content/drive/MyDrive/Automatic Recruitment System/train_set.dat')):
    !unzip -u "/content/drive/MyDrive/Automatic Recruitment System/training_set.zip" -d "/content/drive/MyDrive/Automatic Recruitment System"
  if not(osp.exists('/content/drive/MyDrive/Automatic Recruitment System/valid_set.dat')):
    !unzip -u "/content/drive/MyDrive/Automatic Recruitment System/validation_set.zip" -d "/content/drive/MyDrive/Automatic Recruitment System"
  if not(osp.exists('/content/drive/MyDrive/Automatic Recruitment System/test_set.dat')):
    !unzip -u "/content/drive/MyDrive/Automatic Recruitment System/validation_set.zip/test_set.zip" -d "/content/drive/MyDrive/Automatic Recruitment System"

unzip_files()

def extract_audio_from_video(file_path: str) -> np.ndarray:
    """
    Extract the audio from the video.
    :param file_path: The video file path.
    :return: The audio data.
    """

    inputfile = ff.input(file_path)
    out = inputfile.output('-', format='f32le',
                           acodec='pcm_f32le', ac=1, ar='44100')
    raw = out.run(capture_stdout=True)
    del inputfile, out
    return np.frombuffer(raw[0], np.float32)


def preprocess_audio_series(raw_data: np.ndarray) -> np.ndarray:
    """
    Preprocess the audio series.
    :param raw_data: The raw audio data.
    :return: The preprocessed audio data.
    """

    # Set the number of MFCC(N) and the number of frames(M) to be extracted from the audio data.
    N, M = 24, 1319

    # Extract MFCC from the audio data and convert it to a numpy array.
    mfcc_data = librosa.feature.mfcc(y=raw_data, n_mfcc=N)

    # Standardize MFCC (zero mean and unit variance)
    mfcc_data_standardized = (
        mfcc_data - np.mean(mfcc_data, axis=1, keepdims=True)) / np.std(mfcc_data, axis=1, keepdims=True)

    # Pad the MFCC data with zeros to make it of shape (N, M)
    if mfcc_data_standardized.shape[1] < M:
        padding = np.zeros((N, M - mfcc_data_standardized.shape[1]))
        padded_data = np.hstack((mfcc_data_standardized, padding))
    else:
        padded_data = mfcc_data_standardized[:, :M]

    return padded_data


def get_number_of_frames(file_path: str) -> int:
    """
    Get the number of frames in the video.
    :param file_path: The video file path.
    :return: The number of frames in the video.
    """

    # Get the number of frames in the video.
    probe = ff.probe(file_path)
    video_streams = [stream for stream in probe["streams"]
                     if stream["codec_type"] == "video"]
    # width = video_streams[0]['coded_width']
    # height = video_streams[0]['coded_height']
    del probe
    return video_streams[0]['nb_frames']


def extract_N_video_frames(file_path: str, number_of_samples: int = 6) -> List[np.ndarray]:
    """
    Extract N video frames from the video.
    :param file_path: The video file path.
    :param number_of_samples: The number of video frames to be extracted.
    :return: The list of extracted video frames.
    """

    # Get the number of frames in the video.
    nb_frames = int(get_number_of_frames(file_path=file_path))

    video_frames = []
    random_indexes = random.sample(range(0, nb_frames), number_of_samples)

    # Extract the video frames.
    cap = cv2.VideoCapture(file_path)
    for ind in random_indexes:
        cap.set(1, ind)
        _, frame = cap.read()
        video_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    cap.release()
    del cap, random_indexes
    return video_frames


def resize_image(image: np.ndarray, new_size: Tuple[int, int]) -> np.ndarray:
    """
    Resize the image.
    :param image: The image to be resized.
    """
    return cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)


def crop_image_window(image: np.ndarray, training: bool = True) -> np.ndarray:
    """
    Crop the image window.
    :param image: The image to be cropped.
    :param training: Whether the image is for training or not.
    :return: The cropped image.
    """

    # Crop the image window to be of size (128, 128).
    height, width, _ = image.shape
    if training:
        MAX_N = height - 128
        MAX_M = width - 128
        rand_N_index, rand_M_index = random.randint(
            0, MAX_N), random.randint(0, MAX_M)
        return image[rand_N_index:(rand_N_index+128), rand_M_index:(rand_M_index+128), :]
    else:
        N_index = (height - 128) // 2
        M_index = (width - 128) // 2
        return image[N_index:(N_index+128), M_index:(M_index+128), :]


def preprocessing_input(file_path: str, training: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Preprocess the input data.
    :param file_path: The video file path.
    :param training: Whether the data is for training or not.
    :return: The preprocessed audio, video and label data.
    """
    
    # Audio
    extracted_audio_raw = extract_audio_from_video(file_path=file_path)
    preprocessed_audio = preprocess_audio_series(raw_data=extracted_audio_raw)

    # Video
    sampled = extract_N_video_frames(file_path=file_path, number_of_samples=6)
    resized_images = [resize_image(
        image=im, new_size=(248, 140)) for im in sampled]
    cropped_images = [crop_image_window(
        image=resi, training=training) / 255.0 for resi in resized_images]
    preprocessed_video = np.stack(cropped_images)

    del extracted_audio_raw, sampled, resized_images, cropped_images
    return (preprocessed_audio, preprocessed_video)


def process_data(dataset_type):
    """
    Process the data and save it to a file for later use.
    :param dataset_type: The type of the dataset (train, valid or test).
    """
    dataset = []
    path = 'Big5/' + dataset_type
    gt_path = 'Big5/gt/annotation_' + dataset_type + '.pkl'
    gt = pickle.load(
        open(gt_path), encoding='latin1')
    t1 = datetime.datetime.utcnow()
    for _ in os.listdir(path):
        for filename in os.listdir(path+'/'+_):
            filePath = path+'/' + _ +'/'+filename
            dataset.append(preprocessing_input(
                file_path=filePath, file_name=filename, dictionary=gt, training=False))
    t2 = datetime.datetime.utcnow()
    # Measuring execution time
    print('Elapsed time: ' + str(t2-t1))

    savename = dataset_type + '_set.dat'
    with open(savename, "wb") as f:
        pickle.dump(dataset, f)

def load_data(dataset_type):
    """
    Load the data.
    :param dataset_type: The type of the dataset.
    :return: The dataset.
    """
    set_path = '/content/drive/MyDrive/Automatic Recruitment System/'
    dataset_path = set_path + dataset_type + '_set.dat'
    if not osp.exists(dataset_path):
        process_data(dataset_type=dataset_type)
    with open(dataset_path, "rb") as f:
        dataset = pickle.load(f)
    return dataset

train_set_data = load_data(dataset_type='train')
valid_set_data = load_data(dataset_type='valid')
test_set_data = load_data(dataset_type='test')

# Ground truth for training dataset
gt = pickle.load(
    open("/content/drive/MyDrive/Automatic Recruitment System/annotation_train.pkl", "rb"), encoding='latin1')

extraversion = list(gt['extraversion'].values())
neuroticism = list(gt['neuroticism'].values())
agreeableness = list(gt['agreeableness'].values())
conscientiousness = list(gt['conscientiousness'].values())
openness = list(gt['openness'].values())

# Visualize the trait distribution of training set
df = pd.DataFrame({'extraversion': extraversion, 'neuroticism': neuroticism, 'agreeableness': agreeableness, 'conscientiousness': conscientiousness, 'openness': openness})
df.plot(kind='density', xlim=(0, 1))

# Checking the dataset numbers
print(f"Training set: {len(train_set_data)}")
print(f"Validation set: {len(valid_set_data)}")
print(f"Test set: {len(test_set_data)}")

def shuffle_indexes(dataset):
    """
    Shuffle the indexes of the dataset.
    :param dataset: The dataset.
    :return: The shuffled indexes.
    """
    return random.randint(0, len(dataset)-1)

train_random_index = shuffle_indexes(train_set_data)
valid_random_index = shuffle_indexes(valid_set_data)
test_random_index = shuffle_indexes(test_set_data)

def visualize_audio():
    """
    Visualize the audio part of the dataset.
    """
    shape = train_set_data[train_random_index][0].shape
    mfcc_train = train_set_data[train_random_index][0].reshape(shape[0], shape[1])

    shape = valid_set_data[valid_random_index][0].shape
    mfcc_valid = valid_set_data[valid_random_index][0].reshape(
        shape[0], shape[1])

    shape = test_set_data[test_random_index][0].shape
    mfcc_train = test_set_data[test_random_index][0].reshape(shape[0], shape[1])

    mfccs = [mfcc_train, mfcc_valid, mfcc_train]

    librosa.display.specshow(mfccs[0], x_axis='time', y_axis='mel')
    plt.colorbar()
    plt.show()
  
visualize_audio()

def visualize_video():
    """
    Visualize the video part of the dataset.
    """
    images_to_plot = []
    images_to_plot.extend([(train_set_data[train_random_index][1])[
                          i, :, :, :] for i in range(0, 6)])
    images_to_plot.extend([(valid_set_data[valid_random_index][1])[
                          i, :, :, :] for i in range(0, 6)])
    images_to_plot.extend([(test_set_data[test_random_index][1])[
                          i, :, :, :] for i in range(0, 6)])

    plt.figure(figsize=(18, 11))

    for i in range(18):
        plt.subplot(3, 6, i+1)
        plt.imshow(images_to_plot[i])
    plt.show()
    
visualize_video()

# Visualize the personality distribution of training set, validation set and test set
personality_train = train_set_data[train_random_index][2]
personality_valid = valid_set_data[valid_random_index][2]
personality_test = test_set_data[test_random_index][2]
personalities = ['Extraversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']

print("Training personalities")
for label, value in zip(personalities, personality_train):
    print(label + ': ' + str(value[0]))

print("\nValidation personalities")
for label, value in zip(personalities, personality_valid):
    print(label + ': ' + str(value[0]))

print("\nTest personalities")
for label, value in zip(personalities, personality_test):
    print(label + ': ' + str(value[0]))

def reshape_to_expected_input(dataset: List[Tuple[np.ndarray, np.ndarray, np.ndarray]]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Reshape the dataset to the expected input.
    :param dataset: The dataset to be reshaped.
    :return: The reshaped dataset.
    """
    x0_list = []
    x1_list = []
    y_list = []
    for i in range(0, len(dataset)):
        x0_list.append(dataset[i][0])
        x1_list.append(dataset[i][1])
        y_list.append(dataset[i][2])
    return (np.stack(x0_list), np.stack(x1_list), np.stack(y_list))

# Reshape the dataset to the expected input
train_input = reshape_to_expected_input(dataset=train_set_data)
del train_set_data
validation_input = reshape_to_expected_input(dataset=valid_set_data)
del valid_set_data
test_input = reshape_to_expected_input(dataset=test_set_data)
del test_set_data

# Save the datasets
data_path = '/content/drive/MyDrive/Automatic Recruitment System/combined_data.npz'
np.savez(data_path, 
         X0_train=train_input[0], X1_train=train_input[1], y_train=train_input[2],
         X0_val=validation_input[0], X1_val=validation_input[1], y_val=validation_input[2],
         X0_test=test_input[0], X1_test=test_input[1], y_test=test_input[2])